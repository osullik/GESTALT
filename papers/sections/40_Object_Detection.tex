\section{Object Detection}
\label{section:object}

To maximize dataset coverage, and demonstrate the flexibility of \emph{GESTALT} we support three methods of adding objects. 
In decresing order of reliability, users can add objects by ingesting KML Files that they have manually annotated, crowd-source tags by querying the Open Street Maps API and automatically through the processing of geolocated photos. 
Each object is assigned a confidence score on ingest, reflecting the certainty that the object tagged at that location is actually the object at that location. 
Hand-labelled objects are defaulted to a confidence score of $1$, OSM objects to $0.75$ and objects labelled by the object detector are based on the confidenc score reported by the object detection model. 

\subsection{Ground Truth Hand Labeled Tags}
The first method we use to obtain object tags is by hand labeling them. 
Hand labeled objects are those that have been manually annotated by a trustworthy source, they are assumed to be correctly labeled and correctly positioned. 
The reason we accept hand-labelled objects is to allow for prior manual annotation work to be folded into GESTALT's database. 

For benchmarking purposes we curated the Swan Valley Wineries dataset containing ground truth object tags and their associated locations stored in Keyhole Markup Language\footnote{\href{https://developers.google.com/kml/documentation/kml_tut}{https://developers.google.com/kml/documentation/kml_tut}} (KML).
The object tagging was conducted manually by a single annotator, combining on-the-ground knowledge with manual inspection of satellite imagery, street-view imagery and publicly available photos of the area. 
The objects tagged are representative, not exhaustive. 
The object tags are aligned with ... 
The tagging was conducted using \textit{Google Earth Professional version 7.3\footnote{\href{https://www.google.com/earth/about/versions/}{https://www.google.com/earth/about/versions/}}}
Attributes of the objects (e.g. color, size, material) are recorded in the comments field as key:value pairs.
Each object from the hand-labeled dataset is assigned a confidence score of 1.0, since it is manually identified and tagged.

%NSCH: is the below sentence about the labeling or the module that aggregates all the sources? Or no longer relevant?
%The KML parser leverages the \textit{fastKML}\footnote{\href{https://pypi.org/project/fastkml/}{Fast KML PyPI Repo}} and ingests a KML file divided by region (where each region is a bounding box covering an arbitrary number of locations). 
%The KML Parser extracts the objects into dictionaries organized by location before exporting the files as JSON for future analysis. 

\subsection{OSM Object Tags}
The second method involves leveraging existing OSM object tags.
While businesses, attractions and other 'locations' are commonly annotated by the OSM community, objects are more sparsely tagged, since few people have the patience to manually label apparently inconsequential things like trees, statues, fountains and telephone poles. 
\emph{GESTALT} ingests what object tags do exist in OSM through 
............describe the process here (API?)................
Each object ingested from OSM is assigned a confidence score of 1.0, since these tags are maintained by the open-source community and are subject to some degree of review and .......explain why we have some confidence in them.............


\subsection{Noisy Image-based Tags}
The third, and most important method by which \emph{GESTALT} ingests objects, is through automatic object detection.
Given a set of images and their associated geo data, ...........what is it actually called?........... the Object Detection module uses YOLO~\footnote{\href{https://github.com/ultralytics/ultralytics}{YOLO}} to identify objects in each image. Those objects are then tagged with the geolocation of the image, and stored...........how?........... For our experiments, we pull ..............how many?............ images over ............timeframe.......... and use pre-trained YOLO v.8 to detect objects from 80 classes (based on the COCO dataset.........footnote........). Each object identified is assigned a probability score ...........how does YOLO get that????..........





